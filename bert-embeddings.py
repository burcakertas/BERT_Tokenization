# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11RWZlLMpPv0koyxPLjnFJsluggERfl-n
"""

pip install transformers -q

from transformers import BertModel, BertTokenizer
import torch

"""One of the biggest and most commonly used model of HuggingFace. It is a pretrained model on English language and this model is uncased so it can't determine the difference between english and English"""

model = BertModel.from_pretrained('bert-base-uncased')

sentence = 'Scuba diving is my passion'

"""In this example I used PyTorch for tokenizing."""
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

"""TensorFlow can also be used to tokenize as follows:

from transformers import BertTokenizer, TFBertModel
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = TFBertModel.from_pretrained("bert-base-uncased")
sentence = "Scuba diving is my passion"
encoded_input = tokenizer(sentence, return_tensors='tf')
output = model(encoded_input)

"""

"""tokenize the sentence and obtain the tokens:"""

tokens = tokenizer.tokenize(sentence)

"""Print the tokens to see what we got"""
print(tokens)

"""Now, we add [CLS] and [SEP] tokens"""

tokens = ['[CLS]']+ tokens + ['[SEP]']

"""Again, print the tokens to see what we got after we add the [CLS] and [SEP] tokens"""
print(tokens)


"""Total of 14 tokens. Lenght of the tokens must be the same. Lets say it needs to be 16 just for as an example value. So we need to add paddings."""

tokens = tokens + ['[PAD]'] + ['[PAD]']

len(tokens)

"""New total is 16, we adjusted the lenght of the total. Now we have to tell the models that PAD is just for adjust the lenght of the tokens. This is the concept of ***Attention Mask***"""

attention_mask = [1 if i!= '[PAD]' else 0 for i in tokens]

print(attention_mask)

"""***Unique Token ID***"""
"""Converting tokens to unique ids"""
token_ids = tokenizer.convert_tokens_to_ids(tokens)

print(token_ids)

"""Converting token ids to tensor so that we can work with our model"""
token_ids = torch.tensor(token_ids).unsqueeze(0)

attention_mask = torch.tensor(attention_mask).unsqueeze(0)

"""Embedding the tokens into model and getting the output"""
output = model(token_ids, attention_mask = attention_mask)

print(output)

"""We can see the shape of our output"""
output[0].shape
"""Output is : torch.Size([1, 16, 768]) where 1 represents batch size, 16 represents the sequence lenght and 768 represents hidden size (each of these token(vector) will be on 768 dimension)"""


